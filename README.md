# Solving Cold Start Problem for New Booking Aggregator

![Python](https://img.shields.io/badge/python-3.11-blue.svg)
![Ruff](https://img.shields.io/badge/style-ruff-%23cc66cc.svg?logo=ruff&logoColor=white)
![Shell](https://img.shields.io/badge/shell-scripts-yellow.svg)
![SQL](https://img.shields.io/badge/sql-hive%2Fstandard-red.svg)

An **S25 Big Data Project**

---

## ğŸ“š Table of Contents

- [ğŸ“Œ Contributors](#-contributors)
- [ğŸ’¼ Requirements](#-requirements)
- [ğŸš€ Before You Start](#-before-you-start)
- [âš¡ Quick Start](#-quick-start)
- [ğŸ—‚ï¸ Repository Structure](#ï¸-repository-structure)
- [ğŸ“¬ Contact](#-contact)

---

## ğŸ“Œ Contributors

- Dmitry Beresnev â€” [d.beresnev@innopolis.university](mailto:d.beresnev@innopolis.university)
- Vsevolod Klyushev â€” [v.klyushev@innopolis.university](mailto:v.klyushev@innopolis.university)
- Nikita Yaneev â€” [n.yaneev@innopolis.university](mailto:n.yaneev@innopolis.university)

---

## ğŸ’¼ Requirements

- âœ… Tested on **Linux**
- ğŸ Requires **Python 3.11+**
- ğŸ’½ Requires **HDFS** and **Hive/SQL engines**

---

## ğŸš€ Before You Start

Ensure you have added your PostgreSQL password to `secrets/.psql.pass`

Make sure you have:

- Hive/SQL environment configured
- HDFS available and running if needed

---

## âš¡ Quick Start

You can start via one simple command:

```bash
bash scripts/main.sh
```

---

## ğŸ—‚ï¸ Repository Structure

```text
.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ .gitignore                 # Ignore large raw data files
â”‚
â”œâ”€â”€ output/                        # Output results, plots and images
â”‚   â”œâ”€â”€ codegen_records.java
â”‚   â”œâ”€â”€ evaluation.csv             # Solution metrics
â”‚   â”œâ”€â”€ model1_predictions.csv     # Predictions of Linear regression
â”‚   â”œâ”€â”€ model2_predictions.csv     # Predictions of Random Forest regressor
â”‚   â”œâ”€â”€ q1_*.jpg to q7_*.jpg       # Insight plots
â”‚   â””â”€â”€ q1.csv to q7.csv           # Insight results
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ app.py                     # Model pipeline
â”‚   â”œâ”€â”€ build_data.py              # Script to preprocess or construct data
â”‚   â”œâ”€â”€ build_projectdb.py
â”‚   â”œâ”€â”€ format.sh                  # Formatting script
â”‚   â”œâ”€â”€ get_insights.sh            # Runs analytical SQL insights
â”‚   â”œâ”€â”€ load_data.sh               # Loads data from cloud
â”‚   â”œâ”€â”€ main.sh                    # Master orchestrator script
â”‚   â”œâ”€â”€ partition.sh               # Partitions data for Hive
â”‚   â”œâ”€â”€ prepare_data.ipynb
â”‚   â”œâ”€â”€ project_data_desc.json     # Data schema and description
â”‚   â”œâ”€â”€ send_to_hdfs.sh            # HDFS upload script
â”‚   â”œâ”€â”€ stage1.sh                  # Stage 1 pipeline
â”‚   â”œâ”€â”€ stage2.sh                  # Stage 2 pipeline
â”‚   â””â”€â”€ stage3.sh                  # Stage 3 pipeline
â”‚
â”œâ”€â”€ secrets/                       # Directory for secure configs
â”‚
â”œâ”€â”€ sql/                           # HiveQL and SQL scripts
â”‚   â”œâ”€â”€ create_tables.hql
â”‚   â”œâ”€â”€ create_tables.sql
â”‚   â”œâ”€â”€ import_data.sql
â”‚   â”œâ”€â”€ partition.hql
â”‚   â”œâ”€â”€ q1.hql to q7.hql           # Insight-specific queries
â”‚   â””â”€â”€ test_database.sql
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md                      # This file
```

---

## ğŸ“¬ Contact

If you have any questions, feel free to reach out via the university emails listed above.
